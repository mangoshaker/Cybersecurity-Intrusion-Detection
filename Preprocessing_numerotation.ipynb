{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468b35f-8ab4-45a8-a70f-7d3efd827519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "intrusion_data = pd.read_csv(\"cybersecurity_intrusion_data.csv\", sep=',')\n",
    "\n",
    "# Varirable Separation \n",
    "cols_to_drop = [\n",
    "    'attack_detected',      \n",
    "    'session_id' \n",
    "]\n",
    "\n",
    "X = intrusion_data.drop(cols_to_drop, axis=1)\n",
    "y = intrusion_data['attack_detected']\n",
    "\n",
    "\n",
    "\n",
    "# Split Train/Test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Identification of columns type\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "categorical_features = X.select_dtypes(include='object').columns\n",
    "\n",
    "#Pipeline construction\n",
    "\n",
    "# Standard Scaler\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Encoding by  numerotation (Ordinal)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')), # Gestion des manquants\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)) # Encodage 0, 1, 2...\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Application\n",
    "# Fitting\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Pré-traitement terminé !\")\n",
    "print(f\"Nouvelle taille de X_train : {X_train_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "feature_names = list(numerical_features) + list(categorical_features)\n",
    "\n",
    "# ACP\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_train_processed)\n",
    "\n",
    "# Variance calculus\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Visualisation\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "# Explained Variance\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='blue')\n",
    "ax1.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, color='blue', label='Variance par axe')\n",
    "ax1.axhline(y=0.95, color='r', linestyle=':', label='Treeshold 95%')\n",
    "ax1.set_title('How much information is conserved ?')\n",
    "ax1.set_xlabel('Number of components')\n",
    "ax1.set_ylabel('Cumulated variance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Projection 2D\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "pca_df = pd.DataFrame(data=X_pca[:, :2], columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = y_train.values\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    x='PC1', y='PC2', hue='Target', data=pca_df, \n",
    "    palette={0: 'tab:blue', 1: 'tab:red'}, alpha=0.6, ax=ax2\n",
    ")\n",
    "ax2.set_title(f'Carte des Connexions (PC1: {explained_variance[0]:.1%} | PC2: {explained_variance[1]:.1%})')\n",
    "ax2.set_xlabel('Main  axis 1 (PC1)')\n",
    "ax2.set_ylabel('Main axis 2 (PC2)')\n",
    "\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "ax2.legend(handles, ['Normal', 'Attack'], title=\"Type of Traffic\")\n",
    "\n",
    "# Matrix of points\n",
    "ax3 = fig.add_subplot(gs[1, :]) # Prend toute la largeur du bas\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=[f'PC{i+1}' for i in range(X_train_processed.shape[1])],\n",
    "    index=feature_names\n",
    ")\n",
    "# Heatmap\n",
    "sns.heatmap(loadings.T, annot=True, cmap='coolwarm', center=0, fmt=\".2f\", ax=ax3)\n",
    "ax3.set_title(\"Which variables create axis? (Red = Positive Influence, Bleu = Négative)\")\n",
    "ax3.set_ylabel(\"Main Components (Axis)\")\n",
    "ax3.set_xlabel(\"Original variables\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fe3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code but with less column of the dataset\n",
    "\n",
    "intrusion_data = pd.read_csv(\"cybersecurity_intrusion_data.csv\", sep=',')\n",
    "\n",
    "\n",
    "# Deleting columns\n",
    "cols_to_drop = [\n",
    "    'attack_detected',      \n",
    "    'session_id',           \n",
    "    'protocol_type',       \n",
    "    'unusual_time_access',  \n",
    "    'encryption_used',     \n",
    "    'network_packet_size'   \n",
    "]\n",
    "\n",
    "X = intrusion_data.drop(cols_to_drop, axis=1)\n",
    "y = intrusion_data['attack_detected']\n",
    "\n",
    "\n",
    "\n",
    "# Split Train/Test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Identification of columns types\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "categorical_features = X.select_dtypes(include='object').columns\n",
    "\n",
    "# Pipeline construction \n",
    "\n",
    "# Standard Scaler \n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Numerotation (Ordinal)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')), # Gestion des manquants\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)) # Encodage 0, 1, 2...\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Application\n",
    "# Fitting\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Pré-traitement terminé !\")\n",
    "print(f\"Nouvelle taille de X_train : {X_train_processed.shape}\")\n",
    "\n",
    "feature_names = list(numerical_features) + list(categorical_features)\n",
    "\n",
    "# ACP\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_train_processed)\n",
    "\n",
    "# Variance Calculus\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Visualisation\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "# Explained Variance\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='blue')\n",
    "ax1.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, color='blue', label='Variance par axe')\n",
    "ax1.axhline(y=0.95, color='r', linestyle=':', label='Treeshold 95%')\n",
    "ax1.set_title('How much information is conserved ?')\n",
    "ax1.set_xlabel('Number of components')\n",
    "ax1.set_ylabel('Cumulated variance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "#Projection 2D \n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "pca_df = pd.DataFrame(data=X_pca[:, :2], columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = y_train.values\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    x='PC1', y='PC2', hue='Target', data=pca_df, \n",
    "    palette={0: 'tab:blue', 1: 'tab:red'}, alpha=0.6, ax=ax2\n",
    ")\n",
    "ax2.set_title(f'Carte des Connexions (PC1: {explained_variance[0]:.1%} | PC2: {explained_variance[1]:.1%})')\n",
    "ax2.set_xlabel('Main  axis 1 (PC1)')\n",
    "ax2.set_ylabel('Main axis 2 (PC2)')\n",
    "\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "ax2.legend(handles, ['Normal', 'Attack'], title=\"Type of Traffic\")\n",
    "\n",
    "# Matrix of points\n",
    "ax3 = fig.add_subplot(gs[1, :]) \n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=[f'PC{i+1}' for i in range(X_train_processed.shape[1])],\n",
    "    index=feature_names\n",
    ")\n",
    "# Heatmap\n",
    "sns.heatmap(loadings.T, annot=True, cmap='coolwarm', center=0, fmt=\".2f\", ax=ax3)\n",
    "ax3.set_title(\"Which variables create axis? (Red = Positive Influence, Bleu = Négative)\")\n",
    "ax3.set_ylabel(\"Main Components (Axis)\")\n",
    "ax3.set_xlabel(\"Original variables\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e9d43",
   "metadata": {},
   "source": [
    "## Analysis of PCA on Reduced dataset\n",
    "The variance is distributed almost evenly across the 5 remaining components (approx. 20-25% each). This is unusual and indicates that there is no single \"dominant\" feature. All retained variables contribute equally to the dataset's structure, implying that the information is dense and cannot be easily compressed further without loss.\n",
    "\n",
    "For the projection, we are only choosing PC1 and PC2 because they are the one with the best cumulated variance. So, the projection exhibits a distinct \"barcode\" or vertical striping pattern. This is an artifact of the Ordinal Encoding applied to the categorical feature (likely browser_type), creating discrete columns at x = -1, 0, 1, etc. Within these vertical bands, the \"Normal\" (blue) and \"Attack\" (red) points are heavily overlapped. There is no clear linear boundary separating the two classes. However, we observe that the red points (attacks) tend to reach higher extremes on the Y-axis (PC2), suggesting that attacks are characterized by extreme values in behavioral features (like session_duration or failed_logins).\n",
    "\n",
    "## Analysis of PCA on Full dataset (except the session_id column) \n",
    "The complexity is significantly higher here. It takes about 8 out of 9 components to explain 95% of the variance. This fragmentation confirms that every feature in the original dataset carries unique, non-redundant information.\n",
    "PC1 is almost entirely driven by browser_type (correlation of 1.00).PC2 captures behavioral anomalies (failed_logins, session_duration).\n",
    "Similar to the reduced dataset, we see the \"barcode\" effect driven by the browser type on PC1. The separation remains non-linear: attacks are hidden inside the traffic of each browser type, distinguishable mainly by their vertical spread .\n",
    "\n",
    "## Comparison of the two \n",
    "Comparing the two results highlights the risks of manual feature selection. In the Reduced Dataset, the PCA showed a simplified structure, but the Full Dataset PCA revealed that the variables we removed (protocol_type, encryption_used) actually constituted independent axes of variance (PC8 and PC9). By removing them, we were effectively discarding entire dimensions of the problem, potentially blinding the model to specific attack vectors relying on protocols or encryption types.\n",
    "\n",
    "## Conclusion \n",
    "The visualization of the Full Dataset demonstrates that the problem is highly non-linear. A simple line cannot separate the blue and red points clustered within the vertical bands.The fragmented variance proves that every column is necessary to capture the full picture of network traffic.\n",
    "The strong overlap of classes confirms that linear models (like the baseline Logistic Regression) will underperform. The visual evidence strongly supports the move to non-linear algorithms like Random Forest (which can isolate the specific \"bands\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
